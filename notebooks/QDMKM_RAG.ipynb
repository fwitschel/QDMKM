{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fwitschel/QDMKM/blob/main/notebooks/QDMKM_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Execute this code only if in colab\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "  print(\"Executing in Colab!\")\n",
        "  # Cloning GitHub repository\n",
        "  !git clone https://github.com/fwitschel/QDMKM.git\n",
        "  %cd QDMKM\n"
      ],
      "metadata": {
        "id": "qRHf7ZEBbs5-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyvaSbo_shy-"
      },
      "source": [
        "We install some libraries that we will need later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rqHYkRjADu5B"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community faiss-cpu langchain-anthropic groq rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzM17s7Kp72a"
      },
      "source": [
        "We read the input file into a so-called dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_aGgXCFD4BX",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "emails = pd.read_csv(\"/content/QDMKM/data/cases-emails.csv\")\n",
        "print(emails)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1F3eO8apZnt"
      },
      "source": [
        "In the input file, each case is represented by one row. We create one Document object for each case that contains, as textual content (to be transformed and stored as embedding vectors) the subject of the initial email, followed by the entire text of the conversation. As metadata, we keep the email address of the sender and the notice with which the extension was requested. Later, it can be useful to have quick access to this metadata..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-5-rsZmEXde"
      },
      "outputs": [],
      "source": [
        "import datetime as dt\n",
        "from langchain_core.documents import Document\n",
        "docs = []\n",
        "for index, row in emails.iterrows():\n",
        "    sender = row['sender']\n",
        "    subject = row['subject']\n",
        "    text = subject + \" \" + row['all_text']\n",
        "    notice = row['notice_weeks']\n",
        "    year = dt.datetime.strptime(row['first_date'], '%Y-%m-%d').year\n",
        "    document = Document(\n",
        "        page_content=text,\n",
        "        metadata={\"source\": sender,\"notice\":notice, \"year\":year},\n",
        "        id = index\n",
        "    )\n",
        "    docs.append(document)\n",
        "\n",
        "print(docs[0])\n",
        "\n",
        "# later, when we combine ranks of documents, it will be useful to have a data structure\n",
        "# that maps document indices to document objects:\n",
        "doc_map = {}\n",
        "for doc in docs:\n",
        "  doc_map[doc.id] = doc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we want to index our emails for basic keyword retrieval. For this, we define a function that will do some pro-processing..."
      ],
      "metadata": {
        "id": "3cvY9gzi_nMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess(text):\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # lowercase the tokens\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # stemming\n",
        "    stemmer = SnowballStemmer('german', ignore_stopwords=True)\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "6gCQSE9D_lrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we do the actual pre-processing on our documents. We store the pre-processed texts in a list, in the same order that they have in the document collection (thus, we can use the order to identify the metadata that belongs to a pre-processed text)"
      ],
      "metadata": {
        "id": "Wbflzh0C_62q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "import operator\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "preprocessed_corpus = []\n",
        "for doc in docs:\n",
        "  doc_tokens = preprocess(doc.page_content)\n",
        "  preprocessed_corpus.append(doc_tokens)"
      ],
      "metadata": {
        "id": "Tjw6XXK6_4Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the new case for which would like to retrieve and summarize similar historical cases. We also set the number (topk) of most similar cases to consider"
      ],
      "metadata": {
        "id": "qjaDVfR5AP6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topk = 3\n",
        "query = \"A student got a very late feedback regarding his MRTP that he wants to react to.\"\n",
        "#query = \"A student discovered that she was pregnant soon after starting the thesis proposal. Towards the end of her thesis, the pregancy became complicated and she had to take leave. A sickness certificate is available.\"\n",
        "#query = \"A student needs more time because he had to take over more responsibilities for a new project / mission. His employer assigned him as a project leader and he could not refuse it.\""
      ],
      "metadata": {
        "id": "AfX1HvNIAPLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We build a keyword retrieval object and use it to run our query against all emails and to get a score for each email (hopefully indicating how relevant the email is for the new case)"
      ],
      "metadata": {
        "id": "dyJbUqNWIv1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bm25_retriever = BM25Okapi(preprocessed_corpus)\n",
        "doc_scores = bm25_retriever.get_scores(preprocess(query))\n",
        "\n",
        "# retrieve the documents (including metadata) corresponding to the topk highest scores\n",
        "# by first inserting doc ids and corresponding scores into a dict and then sorting it by\n",
        "# the values (=scores)\n",
        "doc_scores_dict = {}\n",
        "for i in range(len(doc_scores)):\n",
        "  doc_scores_dict[i] = doc_scores[i]\n",
        "sorted_scores = sorted(doc_scores_dict.items(), key=operator.itemgetter(1))"
      ],
      "metadata": {
        "id": "bh9nYAPBItAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After getting the scores, we create a list where we insert pairs of documents (including their metadata) and their scores, only for the topk top-ranked documents..."
      ],
      "metadata": {
        "id": "YAC9yaveI-aA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bm25_results = []\n",
        "for i in range(topk):\n",
        "  (doc_id, score) = sorted_scores[len(docs)-i-1]\n",
        "  cur_doc = docs[doc_id]\n",
        "  bm25_results.append((cur_doc, score))\n",
        "\n",
        "for result in bm25_results:\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "q3ls8piQI94B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKr94dwNpJRN"
      },
      "source": [
        "Now, we take an embeddings model and use it to create embeddings vectors for our email conversations. In the end, such an embedding vector is just a bunch of numbers..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-small-en\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "#chunk_texts = list(map(lambda d: d.page_content, docs))\n",
        "#embeddings = bge_embeddings.embed_documents(chunk_texts)\n",
        "#print(embeddings[0])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9B8N7J7UPGnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We store the embeddings in a vector store"
      ],
      "metadata": {
        "id": "xE2tQxEt94rI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "db = FAISS.from_documents(docs, bge_embeddings)"
      ],
      "metadata": {
        "id": "xG6OLJjxPTRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we retrieve the topk most similar cases using semantic search, i.e. comparison of embeddings."
      ],
      "metadata": {
        "id": "IL26DA8uZH5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_results = db.similarity_search_with_score(query, k=topk)\n",
        "\n",
        "for i in range(topk):\n",
        "  print(semantic_results[i])"
      ],
      "metadata": {
        "id": "5vqolRX3PXSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we combine the results of keyword and semantic search, we also retrieve - for each document that any of the two searches has found - its age. We want to also factor that into the final ranking because old decisions should not impact new decisions as much as more recent ones."
      ],
      "metadata": {
        "id": "Xpc_Ioy9ApSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rank all documents that appear in either semantic search or keyword search results by age\n",
        "ages = {}\n",
        "for (doc,score) in bm25_results:\n",
        "  ages[doc.id] = 2025 - doc.metadata['year']\n",
        "\n",
        "for (doc,score) in semantic_results:\n",
        "  ages[doc.id] = 2025 - doc.metadata['year']\n",
        "\n",
        "sorted_ages = sorted(ages.items(), key=operator.itemgetter(1))\n",
        "print(sorted_ages)"
      ],
      "metadata": {
        "id": "5eHrGG5bkt8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create a final ranking by using weighted reciprocal rang fusion (RRF)"
      ],
      "metadata": {
        "id": "3vFbYOz3BSQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 60\n",
        "weights = {\"bm25\":0.4,\"semantic\":0.4,\"age\":0.2}\n",
        "\n",
        "final_scores = {}\n",
        "bm25_rank = 1\n",
        "for (doc,score) in bm25_results:\n",
        "  final_scores[doc.id] = weights[\"bm25\"]/(k+bm25_rank)\n",
        "  bm25_rank += 1\n",
        "\n",
        "semantic_rank = 1\n",
        "for (doc,score) in semantic_results:\n",
        "  if doc.id in final_scores:\n",
        "    final_scores[doc.id] += weights[\"semantic\"]/(k+semantic_rank)\n",
        "  else:\n",
        "    final_scores[doc.id] = weights[\"semantic\"]/(k+semantic_rank)\n",
        "  semantic_rank += 1\n",
        "\n",
        "age_rank = 1\n",
        "for (doc_id,score) in sorted_ages:\n",
        "  final_scores[doc_id] += weights[\"age\"]/(k+age_rank)\n",
        "\n",
        "\n",
        "sorted_final = sorted(final_scores.items(), key=operator.itemgetter(1))\n",
        "final_results = []\n",
        "for i in range(topk):\n",
        "  (doc_id, score) = sorted_final[len(sorted_final)-i-1]\n",
        "  cur_doc = doc_map[doc_id]\n",
        "  final_results.append((cur_doc, score))\n",
        "\n",
        "for (doc,score) in final_results:\n",
        "  print(doc, score)"
      ],
      "metadata": {
        "id": "L1IiCRQWBZF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8BTPPEbcNiA"
      },
      "source": [
        "Here, we connect to an LLM at Groq. To make it work, please get yourself an API key for GROQ and store it as a key on the left side of this notebook...!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "def llm(groq_client, prompt):\n",
        "  chat_completion = groq_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "  )\n",
        "\n",
        "  return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "qZLFmUZ_YPPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_client = Groq(\n",
        "    api_key=userdata.get('GROQ_API_KEY')\n",
        ")"
      ],
      "metadata": {
        "id": "x9JPwWjLYuJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you instruct the Large Language Model what to do:\n",
        "\n",
        "* In the \"system\" part of the prompt, you explain the general task, including the\n",
        "context (i.e. the retrieved information) that the system should rely on. You can pass the content of the retrieved emails by putting \"{context}\" into this part of the prompt\n",
        "* In the \"query\" part of the prompt, you give instruction to make a decision about the new case (as introduced already above, before the retrieval)"
      ],
      "metadata": {
        "id": "Mk4C7brMbDzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = '\\n\\n'.join(list(map(lambda c: c[0].page_content, final_results)))\n",
        "prompt = f\"\"\"You are an assistant that helps a study dean to decide about students' request for extending the deadline of their master theses.\n",
        "        The current case is described as follows: {query}.\n",
        "        To decide about the current case, the following historical emails seem to be relevant: {context}. Please make a suggestion whether or not\n",
        "        to grant the deadline extension, including a justification that is based on the given context! If possible, please include quotes from the historical emails\"\"\"\n",
        "print(llm(groq_client, prompt))"
      ],
      "metadata": {
        "id": "YVV2T_ufZiNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TWFT5Ew7ZlN0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}