{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Execute this code only if in colab\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "  print(\"Executing in Colab!\")\n",
        "  # Cloning GitHub repository\n",
        "  !git clone https://github.com/fwitschel/WIMA.git\n",
        "  %cd WIMA\n"
      ],
      "metadata": {
        "id": "qRHf7ZEBbs5-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oafU1Ii4TXtp"
      },
      "source": [
        "\n",
        "First, we will install required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Qh7uQdWfTadO"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community pypdf sentence_transformers faiss-cpu langchain-anthropic groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "X9vaOgI7XvMm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/WIMA/data/IRF_Data_small.csv', sep=\";\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6LdVecjSqwP"
      },
      "source": [
        "### Parsing the documents\n",
        "First, we will load the csv document and parse it using Pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDGqxCpKRuRR"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "docs = []\n",
        "for index, row in df.iterrows():\n",
        "    person = row['person']\n",
        "    title = row['dc.title']\n",
        "    document = Document(\n",
        "        page_content= \"Author: \" + str(person) + \", Title: \" + str(title),\n",
        "        metadata={\"source\": \"IRF\"}\n",
        "    )\n",
        "    docs.append(document)\n",
        "\n",
        "print(docs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7srp0X7UNTm"
      },
      "outputs": [],
      "source": [
        "print(docs[0].page_content[0:250])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRhvBTARUIJZ"
      },
      "source": [
        "Once we have the text from the document, we have to split it into smaller chunks. We can use LangChain's available splitters, like CharacterTextSplitter in this case:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIv-DbOgUrrG"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(chunks[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2Q7BArwVT1J"
      },
      "source": [
        "We will be using BGE-small, an opensource embeddings model. We will download it from HuggingFace Hub and run it on all chunks to calculate their vector representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTOW6z0PVYij"
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-small-en\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "chunk_texts = list(map(lambda d: d.page_content, chunks))\n",
        "embeddings = bge_embeddings.embed_documents(chunk_texts)\n",
        "print(embeddings[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYk2fjOqXTMf"
      },
      "source": [
        "Once we have the vector representations for all chunks, we can create an in-memory vector database and store all vectors in it. For this example, we will be using a FAISS database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1sXZZ3vXTiW"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "text_embedding_pairs = zip(chunk_texts, embeddings)\n",
        "db = FAISS.from_embeddings(text_embedding_pairs, bge_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgsgun3CYNyI"
      },
      "source": [
        "The database is now set up. Now, we will be taking queries from the user on this information. In the case of expert search, we expect the user to ask for an area of expertise. Then, we retrieve the top k most similar chunks to that query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bvp4RbF_YNH-"
      },
      "outputs": [],
      "source": [
        "topk = 42\n",
        "query = \"knowledge management\"\n",
        "\n",
        "contexts = db.similarity_search(query, k=topk)\n",
        "\n",
        "for i in range(topk):\n",
        "  print(contexts[i].page_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI5rF1aqZJSU"
      },
      "source": [
        "After retrieving the relevant context, we build a prompt using this information and the user's original query. We will use a Llama model via the Huggingface API:\n",
        "\n",
        "\n",
        "\n",
        "> This example uses Huggingface API to call the model. In order for it to work, remember to set the Secret Variable \"HUGGINGFACE_API_KEY\" to your own Huggingface API Key, or change the model to any of your choice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "def llm(groq_client, prompt):\n",
        "  chat_completion = groq_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "  )\n",
        "\n",
        "  return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "sNoYkKxNbrQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_client = Groq(\n",
        "    api_key=userdata.get('GROQ_API_KEY')\n",
        ")"
      ],
      "metadata": {
        "id": "NPimusDPb6OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = '\\n\\n'.join(list(map(lambda c: c.page_content, contexts)))\n",
        "prompt = f\"\"\"You are an assistant that helps users to find people with a certain expertise, based on their publications.\n",
        "You need to analyse the publications and check who has most relevant publications in the given field! The following context contains several relevant publications;\n",
        "for each publication, the author is provided first: {context}. Please only return internal experts!\n",
        "The field that we are interested in is {query}\"\"\"\n",
        "response = llm(groq_client, prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "a44INQPHcEAf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}