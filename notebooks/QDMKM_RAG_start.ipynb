{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fwitschel/QDMKM/blob/main/notebooks/QDMKM_RAG_start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Execute this code only if in colab\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "  print(\"Executing in Colab!\")\n",
        "  # Cloning GitHub repository\n",
        "  !git clone https://github.com/fwitschel/QDMKM.git\n",
        "  %cd QDMKM\n"
      ],
      "metadata": {
        "id": "qRHf7ZEBbs5-",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86a3c650-ae4c-4552-bd6e-1eb9901972ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing in Colab!\n",
            "Cloning into 'QDMKM'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 77 (delta 13), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (77/77), 79.04 KiB | 2.47 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "/content/QDMKM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyvaSbo_shy-"
      },
      "source": [
        "We install some libraries that we will need later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rqHYkRjADu5B",
        "outputId": "71ccfbf8-5ed4-4ce8-ba3a-42ebf05c8e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting langchain-anthropic\n",
            "  Downloading langchain_anthropic-0.3.21-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting groq\n",
            "  Downloading groq-0.32.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.77)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Collecting anthropic<1.0.0,>=0.69.0 (from langchain-anthropic)\n",
            "  Downloading anthropic-0.69.0-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.69.0->langchain-anthropic) (0.17.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.69.0->langchain-anthropic) (0.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.30-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_anthropic-0.3.21-py3-none-any.whl (32 kB)\n",
            "Downloading groq-0.32.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading anthropic-0.69.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, rank_bm25, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, groq, dataclasses-json, anthropic, langchain-anthropic, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anthropic-0.69.0 dataclasses-json-0.6.7 faiss-cpu-1.12.0 groq-0.32.0 langchain-anthropic-0.3.21 langchain-community-0.3.30 marshmallow-3.26.1 mypy-extensions-1.1.0 rank_bm25-0.2.2 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community faiss-cpu langchain-anthropic groq rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzM17s7Kp72a"
      },
      "source": [
        "We read the input file into a so-called dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_aGgXCFD4BX",
        "outputId": "bbf15806-aae8-4cba-ca4f-ea4258b02fa2",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 sender                 receiver  \\\n",
            "0     student42@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "1     student21@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "2     student84@MBFH.ch  former_msc_dean@MBFH.ch   \n",
            "3    student168@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "4    student336@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "5    student672@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "6     student72@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "7     student67@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "8      student7@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "9    student745@MBFH.ch  former_msc_dean@MBFH.ch   \n",
            "10    student45@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "11    student74@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "12     student5@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "13   student666@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "14   student888@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "15  student4242@MBFH.ch         my_msc_dean@MBFH   \n",
            "\n",
            "                                     subject  first_date  notice_weeks  \\\n",
            "0                        Re: late submission  2019-06-16           1.0   \n",
            "1   Re: extend deadline because of sickness?  2020-06-14           1.0   \n",
            "2                      Re: thesis submission  2016-07-01           NaN   \n",
            "3                  Re: request for extension  2021-05-10           6.0   \n",
            "4                              Re: extension  2019-05-16           5.0   \n",
            "5                               Re:my thesis  2022-04-23           8.0   \n",
            "6                                  Re: help!  2020-06-19           0.0   \n",
            "7                           Re:master thesis  2021-04-18           9.0   \n",
            "8                               Re:my thesis  2018-06-16           0.0   \n",
            "9                              Re:extension?  2015-05-10           6.0   \n",
            "10                              Re:extension  2021-06-15           0.0   \n",
            "11                          Re:master thesis  2020-05-09           6.0   \n",
            "12                    Re:problem with thesis  2019-06-05           2.0   \n",
            "13                       Re:late submission?  2018-06-17           0.0   \n",
            "14                                  Re:help!  2021-04-16           9.0   \n",
            "15                           Re:MT Extension  2023-06-19           0.0   \n",
            "\n",
            "                     tags                                           all_text  \\\n",
            "0    sickness certificate  Dear Ms Smith, unfortunately, we cannot accept...   \n",
            "1    sickness certificate  Dear Mr Doe, your request to extend the deadli...   \n",
            "2                     job  Dear Ms Gross, I am sorry, but you will have t...   \n",
            "3                     job  Dear Mr Klein, we have decided to extend the d...   \n",
            "4                   child  Dear Ms Little, unfortunately we cannot grant ...   \n",
            "5   child family sickness  Dear Ms Big, yes we can extend your submission...   \n",
            "6                 results  Dear Mr Green, I am sorry to hear about these ...   \n",
            "7                 results  Dear Mr Red, I am sorry, but we cannot grant y...   \n",
            "8                 results  Dear Ms Yellow, I am sorry, but we cannot gran...   \n",
            "9                 results  Dear Mr Purple, your request for deadline exte...   \n",
            "10                results  Dear Ms Blue, I am sorry to tell you that it i...   \n",
            "11               sickness  Dear Mr Brown, Although your certificate does ...   \n",
            "12        family sickness  Dear Ms Pink, I am sorry, but without medical ...   \n",
            "13               sickness  Dear Ms Orange, you will be granted an extensi...   \n",
            "14                    job  Dear Mr White, I am sorry to hear about your s...   \n",
            "15               internal  Dear Ms Grey, while this is very unfortunate, ...   \n",
            "\n",
            "   attachment  \n",
            "0         yes  \n",
            "1         yes  \n",
            "2          no  \n",
            "3         yes  \n",
            "4          no  \n",
            "5         yes  \n",
            "6          no  \n",
            "7          no  \n",
            "8          no  \n",
            "9          no  \n",
            "10         no  \n",
            "11        yes  \n",
            "12         no  \n",
            "13        yes  \n",
            "14        yes  \n",
            "15         no  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "emails = pd.read_csv(\"/content/QDMKM/data/cases-emails.csv\")\n",
        "print(emails)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1F3eO8apZnt"
      },
      "source": [
        "In the input file, each case is represented by one row. We create one Document object for each case that contains, as textual content (to be transformed and stored as embedding vectors) the subject of the initial email, followed by the entire text of the conversation. As metadata, we keep the email address of the sender and the notice with which the extension was requested. Later, it can be useful to have quick access to this metadata..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-5-rsZmEXde",
        "outputId": "048bf86a-26a7-4a75-e177-a7a96911c812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Re: late submission Dear Ms Smith, unfortunately, we cannot accept your request for deadline extension. Since your sickness occurred during a non-critical period of your thesis work and was comparatively short, there was enough time to resolve issues resulting from it. We are looking forward to receiving your thesis submission on June 21st. Best regards, The Dean. ---- Dear Prof. Dean, please find attached the certificate for my sickness. Hoping for a positive decision, best regards, Jane Smith. ---- Dear Ms Smith, could you please send us a medical certificate for your sick period. Please note that this does not imply that we will grant the extension, it is just a routine request. Thanks and best regards, The Dean. --- Dear Prof. Dean, I am writing to you to ask for a deadline extension of 1 week for my master thesis. In February, I had a really bad flu from which it took me two weeks to recover. I feel that I am still suffering from the consequences since my whole thesis plan got mixed up during that period - where I was in a great flow before analysing the literature, I felt I had to start from scratch after my sickness. Now, I am behind, but feel that an additional week would allow me to deliver good results. I hope you understand my situatiuon. Thank you and best regards, Jane' metadata={'source': 'student42@MBFH.ch', 'notice': 1.0, 'year': 2019}\n"
          ]
        }
      ],
      "source": [
        "import datetime as dt\n",
        "from langchain_core.documents import Document\n",
        "docs = []\n",
        "for index, row in emails.iterrows():\n",
        "    sender = row['sender']\n",
        "    subject = row['subject']\n",
        "    text = subject + \" \" + row['all_text']\n",
        "    notice = row['notice_weeks']\n",
        "    year = dt.datetime.strptime(row['first_date'], '%Y-%m-%d').year\n",
        "    document = Document(\n",
        "        page_content=text,\n",
        "        metadata={\"source\": sender,\"notice\":notice, \"year\":year},\n",
        "        id = index\n",
        "    )\n",
        "    docs.append(document)\n",
        "\n",
        "print(docs[0])\n",
        "\n",
        "# later, when we combine ranks of documents, it will be useful to have a data structure\n",
        "# that maps document indices to document objects:\n",
        "doc_map = {}\n",
        "for doc in docs:\n",
        "  doc_map[doc.id] = doc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we want to index our emails for basic keyword retrieval. For this, we define a function that will do some pro-processing..."
      ],
      "metadata": {
        "id": "3cvY9gzi_nMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess(text):\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # lowercase the tokens\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # stemming\n",
        "    stemmer = SnowballStemmer('german', ignore_stopwords=True)\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "6gCQSE9D_lrD",
        "outputId": "d1a9fd04-6b1a-4717-efa9-92317ae9861d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we do the actual pre-processing on our documents. We store the pre-processed texts in a list, in the same order that they have in the document collection (thus, we can use the order to identify the metadata that belongs to a pre-processed text)"
      ],
      "metadata": {
        "id": "Wbflzh0C_62q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "import operator\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "preprocessed_corpus = []\n",
        "for doc in docs:\n",
        "  doc_tokens = preprocess(doc.page_content)\n",
        "  preprocessed_corpus.append(doc_tokens)"
      ],
      "metadata": {
        "id": "Tjw6XXK6_4Ki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f8a966-de6b-438e-a802-03d3a0d61cd0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the new case for which would like to retrieve and summarize similar historical cases. We also set the number (topk) of most similar cases to consider"
      ],
      "metadata": {
        "id": "qjaDVfR5AP6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topk = 3\n",
        "query = \"A student got a very late feedback regarding his MRTP that he wants to react to.\"\n",
        "#query = \"A student discovered that she was pregnant soon after starting the thesis proposal. Towards the end of her thesis, the pregancy became complicated and she had to take leave. A sickness certificate is available.\"\n",
        "#query = \"A student needs more time because he had to take over more responsibilities for a new project / mission. His employer assigned him as a project leader and he could not refuse it.\""
      ],
      "metadata": {
        "id": "AfX1HvNIAPLa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We build a keyword retrieval object and use it to run our query against all emails and to get a score for each email (hopefully indicating how relevant the email is for the new case)"
      ],
      "metadata": {
        "id": "dyJbUqNWIv1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bm25_retriever = BM25Okapi(preprocessed_corpus)\n",
        "doc_scores = bm25_retriever.get_scores(preprocess(query))\n",
        "\n",
        "# retrieve the documents (including metadata) corresponding to the topk highest scores\n",
        "# by first inserting doc ids and corresponding scores into a dict and then sorting it by\n",
        "# the values (=scores)\n",
        "doc_scores_dict = {}\n",
        "for i in range(len(doc_scores)):\n",
        "  doc_scores_dict[i] = doc_scores[i]\n",
        "sorted_scores = sorted(doc_scores_dict.items(), key=operator.itemgetter(1))"
      ],
      "metadata": {
        "id": "bh9nYAPBItAy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After getting the scores, we create a list where we insert pairs of documents (including their metadata) and their scores, only for the topk top-ranked documents..."
      ],
      "metadata": {
        "id": "YAC9yaveI-aA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bm25_results = []\n",
        "for i in range(topk):\n",
        "  (doc_id, score) = sorted_scores[len(docs)-i-1]\n",
        "  cur_doc = docs[doc_id]\n",
        "  bm25_results.append((cur_doc, score))\n",
        "\n",
        "for result in bm25_results:\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "q3ls8piQI94B",
        "outputId": "f3cd7ff9-9af9-4552-fdd8-7f3f254f0275",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Document(id='15', metadata={'source': 'student4242@MBFH.ch', 'notice': 0.0, 'year': 2023}, page_content='Re:MT Extension Dear Ms Grey, while this is very unfortunate, it is by no means certain that the consequences that you fear will arise. Please talk to your supervisor and clarify how to solve this. Without a feedback from her, I am not willing to grant an extension. Best regards, The Dean ---- Dear Dean, I am sorry to bother you about this, but I am really desperate: because my supervisor was sick for a long time in winter, I did not receive an MTRP assessment until now. Now, I got one and there is a point that I should correct that my supervisor sees as essential and I do not have the time to work it out now. I am afraid to get a very low grade if I do not! Could you grant me an extension by one or two weeks so that I can work this in? Thank you!'), np.float64(8.731743609582894))\n",
            "(Document(id='9', metadata={'source': 'student745@MBFH.ch', 'notice': 6.0, 'year': 2015}, page_content='Re:extension? Dear Mr Purple, your request for deadline extension can be accepted because of the exceptional circumstances that were beyond your control. Please submit your thesis until August 4 midnight. Best regards, The Dean ---- Dear Ms FormerDean, can you please consider extending the deadline of my master thesis? I am working with a company and since the beginning, the collaboration was difficult. Interviews had to be re-scheduled several times and when it was finally agreed which data to use for my analysis (which was only in February), it took the company another two months to make the data available. I wrote them reminders all the time and tried to adapt my plan, but they always told me that it would arrive soon etc. In the end, I received the data in late April - and this was when I could actually start working on the thesis. I also had to clarify some questions regarding the data - again, I had to wait for weeks for a response, during which time I was again blocked. Now, everything is clarified and I feel that with another two weeks time, I will be able to successfully complete the thesis. Thanks for your support in advance, best regards, Pete Purple'), np.float64(7.03702863143313))\n",
            "(Document(id='6', metadata={'source': 'student72@MBFH.ch', 'notice': 0.0, 'year': 2020}, page_content='Re: help! Dear Mr Green, I am sorry to hear about these problems. However, your case is not one where we can grant an extension - it comes very late and the problem was known for a long time so that you could have taken appropriate countermeasures in due time (e.g. using a different method instead of a survey). We are looking forward to receiving your master thesis on Thursday. Best regards, The Dean ---- Dear Prof, I would like to ask for a deadline extension for my master thesis. I really got into trouble because of the poor response rate for the survey that I did in the beginning of my thesis. It meant that I had to do the survey again and I spent a lot of time convincing people to participate. All this was not planned... Would be great to hear from you, best regards, Greg'), np.float64(6.866302873955778))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKr94dwNpJRN"
      },
      "source": [
        "Now, we take an embeddings model and use it to create embeddings vectors for our email conversations. In the end, such an embedding vector is just a bunch of numbers..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-small-en\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "#chunk_texts = list(map(lambda d: d.page_content, docs))\n",
        "#embeddings = bge_embeddings.embed_documents(chunk_texts)\n",
        "#print(embeddings[0])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9B8N7J7UPGnv",
        "outputId": "12d635cd-9624-405a-e291-adfeab8384b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3020136791.py:6: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  bge_embeddings = HuggingFaceBgeEmbeddings(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "871099f3a4ed4897860e9179932f7f45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2398514b1da74fad9f90f4c1fb54480c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48ffb2c94b024aa2aa462f61176228f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d9d3716d10b4a8181e97ae8e9823294"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8f6aa0899ba4729ab463a6fd633a4ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2751b37eda2943a0a26053927a35c0da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dda522d9c57a44799c63bc49557ea2fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cbdf0ed2bc74aa8af8d40526eb877a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6112f9b829744c8d81c935c277655963"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ead1b97af4f340089d9a2d1d1f82e9ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f601765d00d64c96b6563d94be356627"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We store the embeddings in a vector store"
      ],
      "metadata": {
        "id": "xE2tQxEt94rI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "db = FAISS.from_documents(docs, bge_embeddings)"
      ],
      "metadata": {
        "id": "xG6OLJjxPTRF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we retrieve the topk most similar cases using semantic search, i.e. comparison of embeddings."
      ],
      "metadata": {
        "id": "IL26DA8uZH5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_results = db.similarity_search_with_score(query, k=topk)\n",
        "\n",
        "for i in range(topk):\n",
        "  print(semantic_results[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vqolRX3PXSN",
        "outputId": "6b2f7502-442b-4fdb-d579-af912556fb26"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Document(id='0', metadata={'source': 'student42@MBFH.ch', 'notice': 1.0, 'year': 2019}, page_content='Re: late submission Dear Ms Smith, unfortunately, we cannot accept your request for deadline extension. Since your sickness occurred during a non-critical period of your thesis work and was comparatively short, there was enough time to resolve issues resulting from it. We are looking forward to receiving your thesis submission on June 21st. Best regards, The Dean. ---- Dear Prof. Dean, please find attached the certificate for my sickness. Hoping for a positive decision, best regards, Jane Smith. ---- Dear Ms Smith, could you please send us a medical certificate for your sick period. Please note that this does not imply that we will grant the extension, it is just a routine request. Thanks and best regards, The Dean. --- Dear Prof. Dean, I am writing to you to ask for a deadline extension of 1 week for my master thesis. In February, I had a really bad flu from which it took me two weeks to recover. I feel that I am still suffering from the consequences since my whole thesis plan got mixed up during that period - where I was in a great flow before analysing the literature, I felt I had to start from scratch after my sickness. Now, I am behind, but feel that an additional week would allow me to deliver good results. I hope you understand my situatiuon. Thank you and best regards, Jane'), np.float32(0.31001335))\n",
            "(Document(id='6', metadata={'source': 'student72@MBFH.ch', 'notice': 0.0, 'year': 2020}, page_content='Re: help! Dear Mr Green, I am sorry to hear about these problems. However, your case is not one where we can grant an extension - it comes very late and the problem was known for a long time so that you could have taken appropriate countermeasures in due time (e.g. using a different method instead of a survey). We are looking forward to receiving your master thesis on Thursday. Best regards, The Dean ---- Dear Prof, I would like to ask for a deadline extension for my master thesis. I really got into trouble because of the poor response rate for the survey that I did in the beginning of my thesis. It meant that I had to do the survey again and I spent a lot of time convincing people to participate. All this was not planned... Would be great to hear from you, best regards, Greg'), np.float32(0.31700632))\n",
            "(Document(id='13', metadata={'source': 'student666@MBFH.ch', 'notice': 0.0, 'year': 2018}, page_content='Re:late submission? Dear Ms Orange, you will be granted an extension of two weeks. We understand the critically of the point in time when your sickness occurred. Please submit your thesis on August 4th, midnight. Regards, The Dean ---- Hi Dean, as you can see from the attached medical certificate, I was sick for more than two weeks and would like to ask for a deadline extension for my master thesis. The sickness started three weeks ago when I was starting to write up my results. There were also two (out of 5) evaluation interviews that I had to cancel because of the sickness. This means that I was unable to finish the thesis. Hoping for your understanding, best regards, Olivia'), np.float32(0.3171185))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we combine the results of keyword and semantic search, we also retrieve - for each document that any of the two searches has found - its age. We want to also factor that into the final ranking because old decisions should not impact new decisions as much as more recent ones."
      ],
      "metadata": {
        "id": "Xpc_Ioy9ApSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rank all documents that appear in either semantic search or keyword search results by age\n",
        "ages = {}\n",
        "for (doc,score) in bm25_results:\n",
        "  ages[doc.id] = 2025 - doc.metadata['year']\n",
        "\n",
        "for (doc,score) in semantic_results:\n",
        "  ages[doc.id] = 2025 - doc.metadata['year']\n",
        "\n",
        "sorted_ages = sorted(ages.items(), key=operator.itemgetter(1))\n",
        "print(sorted_ages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eHrGG5bkt8H",
        "outputId": "081f2931-8e92-4f12-b739-610d05250376"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('15', 2), ('6', 5), ('0', 6), ('13', 7), ('9', 10)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create a final ranking by using weighted reciprocal rang fusion (RRF)"
      ],
      "metadata": {
        "id": "3vFbYOz3BSQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 60\n",
        "weights = {\"bm25\":0.4,\"semantic\":0.4,\"age\":0.2}\n",
        "\n",
        "final_scores = {}\n",
        "bm25_rank = 1\n",
        "for (doc,score) in bm25_results:\n",
        "  final_scores[doc.id] = weights[\"bm25\"]/(k+bm25_rank)\n",
        "  bm25_rank += 1\n",
        "\n",
        "semantic_rank = 1\n",
        "for (doc,score) in semantic_results:\n",
        "  if doc.id in final_scores:\n",
        "    final_scores[doc.id] += weights[\"semantic\"]/(k+semantic_rank)\n",
        "  else:\n",
        "    final_scores[doc.id] = weights[\"semantic\"]/(k+semantic_rank)\n",
        "  semantic_rank += 1\n",
        "\n",
        "age_rank = 1\n",
        "for (doc_id,score) in sorted_ages:\n",
        "  final_scores[doc_id] += weights[\"age\"]/(k+age_rank)\n",
        "\n",
        "\n",
        "sorted_final = sorted(final_scores.items(), key=operator.itemgetter(1))\n",
        "final_results = []\n",
        "for i in range(topk):\n",
        "  (doc_id, score) = sorted_final[len(sorted_final)-i-1]\n",
        "  cur_doc = doc_map[doc_id]\n",
        "  final_results.append((cur_doc, score))\n",
        "\n",
        "for (doc,score) in final_results:\n",
        "  print(doc, score)"
      ],
      "metadata": {
        "id": "L1IiCRQWBZF0",
        "outputId": "5461d97b-6c24-492b-8941-1fbf75133722",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Re: help! Dear Mr Green, I am sorry to hear about these problems. However, your case is not one where we can grant an extension - it comes very late and the problem was known for a long time so that you could have taken appropriate countermeasures in due time (e.g. using a different method instead of a survey). We are looking forward to receiving your master thesis on Thursday. Best regards, The Dean ---- Dear Prof, I would like to ask for a deadline extension for my master thesis. I really got into trouble because of the poor response rate for the survey that I did in the beginning of my thesis. It meant that I had to do the survey again and I spent a lot of time convincing people to participate. All this was not planned... Would be great to hear from you, best regards, Greg' metadata={'source': 'student72@MBFH.ch', 'notice': 0.0, 'year': 2020} 0.01607950777702232\n",
            "page_content='Re: late submission Dear Ms Smith, unfortunately, we cannot accept your request for deadline extension. Since your sickness occurred during a non-critical period of your thesis work and was comparatively short, there was enough time to resolve issues resulting from it. We are looking forward to receiving your thesis submission on June 21st. Best regards, The Dean. ---- Dear Prof. Dean, please find attached the certificate for my sickness. Hoping for a positive decision, best regards, Jane Smith. ---- Dear Ms Smith, could you please send us a medical certificate for your sick period. Please note that this does not imply that we will grant the extension, it is just a routine request. Thanks and best regards, The Dean. --- Dear Prof. Dean, I am writing to you to ask for a deadline extension of 1 week for my master thesis. In February, I had a really bad flu from which it took me two weeks to recover. I feel that I am still suffering from the consequences since my whole thesis plan got mixed up during that period - where I was in a great flow before analysing the literature, I felt I had to start from scratch after my sickness. Now, I am behind, but feel that an additional week would allow me to deliver good results. I hope you understand my situatiuon. Thank you and best regards, Jane' metadata={'source': 'student42@MBFH.ch', 'notice': 1.0, 'year': 2019} 0.009836065573770491\n",
            "page_content='Re:MT Extension Dear Ms Grey, while this is very unfortunate, it is by no means certain that the consequences that you fear will arise. Please talk to your supervisor and clarify how to solve this. Without a feedback from her, I am not willing to grant an extension. Best regards, The Dean ---- Dear Dean, I am sorry to bother you about this, but I am really desperate: because my supervisor was sick for a long time in winter, I did not receive an MTRP assessment until now. Now, I got one and there is a point that I should correct that my supervisor sees as essential and I do not have the time to work it out now. I am afraid to get a very low grade if I do not! Could you grant me an extension by one or two weeks so that I can work this in? Thank you!' metadata={'source': 'student4242@MBFH.ch', 'notice': 0.0, 'year': 2023} 0.009836065573770491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8BTPPEbcNiA"
      },
      "source": [
        "Here, we connect to an LLM at Groq. To make it work, please get yourself an API key for GROQ and store it as a key on the left side of this notebook...!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "def llm(groq_client, prompt):\n",
        "  chat_completion = groq_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "  )\n",
        "\n",
        "  return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "qZLFmUZ_YPPU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_client = Groq(\n",
        "    api_key=userdata.get('GROQ_API_KEY')\n",
        ")"
      ],
      "metadata": {
        "id": "x9JPwWjLYuJ6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you instruct the Large Language Model what to do:\n",
        "\n",
        "* In the \"system\" part of the prompt, you explain the general task, including the\n",
        "context (i.e. the retrieved information) that the system should rely on. You can pass the content of the retrieved emails by putting \"{context}\" into this part of the prompt\n",
        "* In the \"query\" part of the prompt, you give instruction to make a decision about the new case (as introduced already above, before the retrieval)"
      ],
      "metadata": {
        "id": "Mk4C7brMbDzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = '\\n\\n'.join(list(map(lambda c: c[0].page_content, final_results)))\n",
        "prompt = f\"\"\"You are an assistant that helps a study dean to decide about students' request for extending the deadline of their master theses.\n",
        "        The current case is described as follows: {query}.\n",
        "        To decide about the current case, the following historical emails seem to be relevant: {context}. Please make a suggestion whether or not\n",
        "        to grant the deadline extension, including a justification that is based on the given context! If possible, please include quotes from the historical emails\"\"\"\n",
        "print(llm(groq_client, prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVV2T_ufZiNs",
        "outputId": "fd6cfb44-164d-4d2c-937e-b071b2970087"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the given context, I suggest granting the deadline extension. The student's situation is unprecedented, as they received very late feedback regarding their MRTP, which they want to react to. This is not a case where the student had ample time to address the issue, as seen in previous cases.\n",
            "\n",
            "For example, in the case of Mr. Green, the dean stated, \"your case is not one where we can grant an extension - it comes very late and the problem was known for a long time so that you could have taken appropriate countermeasures in due time.\" In contrast, the current student received feedback at a very late stage, making it impossible for them to take countermeasures earlier.\n",
            "\n",
            "Similarly, in the case of Ms. Smith, the dean noted that her sickness occurred during a non-critical period of her thesis work and was comparatively short, allowing her to resolve issues resulting from it. However, the current student's situation is different, as the late feedback has put them in a difficult position, making it hard to deliver good results within the original deadline.\n",
            "\n",
            "The dean's response to Ms. Grey's request also highlights the importance of supervisor feedback. The dean stated, \"Without a feedback from her, I am not willing to grant an extension.\" In the current case, the student has received feedback from their supervisor, who sees the correction as essential. This emphasizes the need for the student to address the issue to avoid a potentially low grade.\n",
            "\n",
            "Given the exceptional circumstances, I recommend granting a short deadline extension, ideally one to two weeks, to allow the student to work on the correction. This would be in line with the dean's goal of ensuring students can deliver good results, as stated in Ms. Smith's case: \"we are looking forward to receiving your thesis submission on June 21st\" - implying that the dean wants students to submit quality work.\n",
            "\n",
            "In the words of the student, \"I am afraid to get a very low grade if I do not\" have time to work on the correction. Granting an extension would demonstrate empathy and understanding of the student's situation, allowing them to submit a thesis that meets the expected standards.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TWFT5Ew7ZlN0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
