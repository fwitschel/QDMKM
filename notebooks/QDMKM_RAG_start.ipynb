{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fwitschel/QDMKM/blob/main/notebooks/QDMKM_RAG_start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Execute this code only if in colab\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "  print(\"Executing in Colab!\")\n",
        "  # Cloning GitHub repository\n",
        "  !git clone https://github.com/fwitschel/QDMKM.git\n",
        "  %cd QDMKM\n"
      ],
      "metadata": {
        "id": "qRHf7ZEBbs5-",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b1ec34b-8b7e-4705-f2fe-69216fffa8ae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing in Colab!\n",
            "Cloning into 'QDMKM'...\n",
            "remote: Enumerating objects: 94, done.\u001b[K\n",
            "remote: Counting objects: 100% (94/94), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 94 (delta 22), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (94/94), 92.24 KiB | 4.19 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n",
            "/content/QDMKM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyvaSbo_shy-"
      },
      "source": [
        "We install some libraries that we will need later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rqHYkRjADu5B",
        "outputId": "7be8165c-4201-49c5-81c6-a59b7b76add0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting langchain-anthropic\n",
            "  Downloading langchain_anthropic-0.3.21-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting groq\n",
            "  Downloading groq-0.32.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.77)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Collecting anthropic<1.0.0,>=0.69.0 (from langchain-anthropic)\n",
            "  Downloading anthropic-0.69.0-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.69.0->langchain-anthropic) (0.17.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.69.0->langchain-anthropic) (0.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.30-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_anthropic-0.3.21-py3-none-any.whl (32 kB)\n",
            "Downloading groq-0.32.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading anthropic-0.69.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, rank_bm25, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, groq, dataclasses-json, anthropic, langchain-anthropic, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anthropic-0.69.0 dataclasses-json-0.6.7 faiss-cpu-1.12.0 groq-0.32.0 langchain-anthropic-0.3.21 langchain-community-0.3.30 marshmallow-3.26.1 mypy-extensions-1.1.0 rank_bm25-0.2.2 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community faiss-cpu langchain-anthropic groq rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzM17s7Kp72a"
      },
      "source": [
        "We read the input file into a so-called dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_aGgXCFD4BX",
        "outputId": "83707715-b633-4843-b41c-0a59b23051de",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 sender                 receiver  \\\n",
            "0     student42@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "1     student21@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "2     student84@MBFH.ch  former_msc_dean@MBFH.ch   \n",
            "3    student168@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "4    student336@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "5    student672@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "6     student72@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "7     student67@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "8      student7@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "9    student745@MBFH.ch  former_msc_dean@MBFH.ch   \n",
            "10    student45@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "11    student74@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "12     student5@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "13   student666@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "14   student888@MBFH.ch      my_msc_dean@MBFH.ch   \n",
            "15  student4242@MBFH.ch         my_msc_dean@MBFH   \n",
            "\n",
            "                                     subject  first_date  notice_weeks  \\\n",
            "0                        Re: late submission  2019-06-16           1.0   \n",
            "1   Re: extend deadline because of sickness?  2020-06-14           1.0   \n",
            "2                      Re: thesis submission  2016-07-01           NaN   \n",
            "3                  Re: request for extension  2021-05-10           6.0   \n",
            "4                              Re: extension  2019-05-16           5.0   \n",
            "5                               Re:my thesis  2022-04-23           8.0   \n",
            "6                                  Re: help!  2020-06-19           0.0   \n",
            "7                           Re:master thesis  2021-04-18           9.0   \n",
            "8                               Re:my thesis  2018-06-16           0.0   \n",
            "9                              Re:extension?  2015-05-10           6.0   \n",
            "10                              Re:extension  2021-06-15           0.0   \n",
            "11                          Re:master thesis  2020-05-09           6.0   \n",
            "12                    Re:problem with thesis  2019-06-05           2.0   \n",
            "13                       Re:late submission?  2018-06-17           0.0   \n",
            "14                                  Re:help!  2021-04-16           9.0   \n",
            "15                           Re:MT Extension  2023-06-19           0.0   \n",
            "\n",
            "                     tags                                           all_text  \\\n",
            "0    sickness certificate  Dear Ms Smith, unfortunately, we cannot accept...   \n",
            "1    sickness certificate  Dear Mr Doe, your request to extend the deadli...   \n",
            "2                     job  Dear Ms Gross, I am sorry, but you will have t...   \n",
            "3                     job  Dear Mr Klein, we have decided to extend the d...   \n",
            "4                   child  Dear Ms Little, unfortunately we cannot grant ...   \n",
            "5   child family sickness  Dear Ms Big, yes we can extend your submission...   \n",
            "6                 results  Dear Mr Green, I am sorry to hear about these ...   \n",
            "7                 results  Dear Mr Red, I am sorry, but we cannot grant y...   \n",
            "8                 results  Dear Ms Yellow, I am sorry, but we cannot gran...   \n",
            "9                 results  Dear Mr Purple, your request for deadline exte...   \n",
            "10                results  Dear Ms Blue, I am sorry to tell you that it i...   \n",
            "11               sickness  Dear Mr Brown, Although your certificate does ...   \n",
            "12        family sickness  Dear Ms Pink, I am sorry, but without medical ...   \n",
            "13               sickness  Dear Ms Orange, you will be granted an extensi...   \n",
            "14                    job  Dear Mr White, I am sorry to hear about your s...   \n",
            "15               internal  Dear Ms Grey, while this is very unfortunate, ...   \n",
            "\n",
            "   attachment  \n",
            "0         yes  \n",
            "1         yes  \n",
            "2          no  \n",
            "3         yes  \n",
            "4          no  \n",
            "5         yes  \n",
            "6          no  \n",
            "7          no  \n",
            "8          no  \n",
            "9          no  \n",
            "10         no  \n",
            "11        yes  \n",
            "12         no  \n",
            "13        yes  \n",
            "14        yes  \n",
            "15         no  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "emails = pd.read_csv(\"/content/QDMKM/data/cases-emails.csv\")\n",
        "print(emails)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1F3eO8apZnt"
      },
      "source": [
        "In the input file, each case is represented by one row. We create one Document object for each case that contains, as textual content (to be transformed and stored as embedding vectors) the subject of the initial email, followed by the entire text of the conversation. As metadata, we keep the email address of the sender and the notice with which the extension was requested. Later, it can be useful to have quick access to this metadata..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-5-rsZmEXde",
        "outputId": "871c43f8-4d07-4a63-a84d-aaf13aa45362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Re: late submission Dear Ms Smith, unfortunately, we cannot accept your request for deadline extension. Since your sickness occurred during a non-critical period of your thesis work and was comparatively short, there was enough time to resolve issues resulting from it. We are looking forward to receiving your thesis submission on June 21st. Best regards, The Dean. ---- Dear Prof. Dean, please find attached the certificate for my sickness. Hoping for a positive decision, best regards, Jane Smith. ---- Dear Ms Smith, could you please send us a medical certificate for your sick period. Please note that this does not imply that we will grant the extension, it is just a routine request. Thanks and best regards, The Dean. --- Dear Prof. Dean, I am writing to you to ask for a deadline extension of 1 week for my master thesis. In February, I had a really bad flu from which it took me two weeks to recover. I feel that I am still suffering from the consequences since my whole thesis plan got mixed up during that period - where I was in a great flow before analysing the literature, I felt I had to start from scratch after my sickness. Now, I am behind, but feel that an additional week would allow me to deliver good results. I hope you understand my situatiuon. Thank you and best regards, Jane' metadata={'source': 'student42@MBFH.ch', 'notice': 1.0, 'year': 2019}\n"
          ]
        }
      ],
      "source": [
        "import datetime as dt\n",
        "from langchain_core.documents import Document\n",
        "docs = []\n",
        "for index, row in emails.iterrows():\n",
        "    sender = row['sender']\n",
        "    subject = row['subject']\n",
        "    text = subject + \" \" + row['all_text']\n",
        "    notice = row['notice_weeks']\n",
        "    year = dt.datetime.strptime(row['first_date'], '%Y-%m-%d').year\n",
        "    document = Document(\n",
        "        page_content=text,\n",
        "        metadata={\"source\": sender,\"notice\":notice, \"year\":year},\n",
        "        id = index\n",
        "    )\n",
        "    docs.append(document)\n",
        "\n",
        "print(docs[0])\n",
        "\n",
        "# later, when we combine ranks of documents, it will be useful to have a data structure\n",
        "# that maps document indices to document objects:\n",
        "doc_map = {}\n",
        "for doc in docs:\n",
        "  doc_map[doc.id] = doc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the new case for which would like to retrieve and summarize similar historical cases. We also set the number (topk) of most similar cases to consider"
      ],
      "metadata": {
        "id": "qjaDVfR5AP6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topk = 3\n",
        "query = \"A student got a very late feedback regarding his MRTP that he wants to react to.\"\n",
        "#query = \"A student discovered that she was pregnant soon after starting the thesis proposal. Towards the end of her thesis, the pregancy became complicated and she had to take leave. A sickness certificate is available.\"\n",
        "#query = \"A student needs more time because he had to take over more responsibilities for a new project / mission. His employer assigned him as a project leader and he could not refuse it.\""
      ],
      "metadata": {
        "id": "AfX1HvNIAPLa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now create embeddings out of our emails that can be stored to and retrieved from a vector store"
      ],
      "metadata": {
        "id": "qvo1QJKt1syc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-small-en\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "#chunk_texts = list(map(lambda d: d.page_content, docs))\n",
        "#embeddings = bge_embeddings.embed_documents(chunk_texts)\n",
        "#print(embeddings[0])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9B8N7J7UPGnv",
        "outputId": "78ab2e42-e08f-4239-a9af-be64537f9f73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3020136791.py:6: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  bge_embeddings = HuggingFaceBgeEmbeddings(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "18ed4a0b7f944f73961b6a09dd1103e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e56c305cdf87493a8655e8f8388af5e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c245c4016174ff1b6c44cfc6a2a076f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "354c7f0f411641f7a7b5f0cb9c4caec4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a570c07a3ed4043896a95971ffffb40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b611788e7c04676aa9508e7afe4189f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8cb207a2b2e4c29a5e7beb674f82870"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49bad9e6f7ba48ae8de38b96bf7b3cf4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51348af625134ceba9302b3023cb5af0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b2fccee8cd04d8b8ec89a240ae4afa0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83344a45bb864677bcdfc6399b2a6214"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We store the embeddings in a vector store"
      ],
      "metadata": {
        "id": "xE2tQxEt94rI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "db = FAISS.from_documents(docs, bge_embeddings)"
      ],
      "metadata": {
        "id": "xG6OLJjxPTRF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we retrieve the topk most similar cases using semantic search, i.e. comparison of embeddings."
      ],
      "metadata": {
        "id": "IL26DA8uZH5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_results = db.similarity_search_with_score(query, k=topk)\n",
        "\n",
        "for i in range(topk):\n",
        "  print(semantic_results[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vqolRX3PXSN",
        "outputId": "bb189cbc-dd20-4514-9165-d3afa0e66c92"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Document(id='0', metadata={'source': 'student42@MBFH.ch', 'notice': 1.0, 'year': 2019}, page_content='Re: late submission Dear Ms Smith, unfortunately, we cannot accept your request for deadline extension. Since your sickness occurred during a non-critical period of your thesis work and was comparatively short, there was enough time to resolve issues resulting from it. We are looking forward to receiving your thesis submission on June 21st. Best regards, The Dean. ---- Dear Prof. Dean, please find attached the certificate for my sickness. Hoping for a positive decision, best regards, Jane Smith. ---- Dear Ms Smith, could you please send us a medical certificate for your sick period. Please note that this does not imply that we will grant the extension, it is just a routine request. Thanks and best regards, The Dean. --- Dear Prof. Dean, I am writing to you to ask for a deadline extension of 1 week for my master thesis. In February, I had a really bad flu from which it took me two weeks to recover. I feel that I am still suffering from the consequences since my whole thesis plan got mixed up during that period - where I was in a great flow before analysing the literature, I felt I had to start from scratch after my sickness. Now, I am behind, but feel that an additional week would allow me to deliver good results. I hope you understand my situatiuon. Thank you and best regards, Jane'), np.float32(0.31001335))\n",
            "(Document(id='6', metadata={'source': 'student72@MBFH.ch', 'notice': 0.0, 'year': 2020}, page_content='Re: help! Dear Mr Green, I am sorry to hear about these problems. However, your case is not one where we can grant an extension - it comes very late and the problem was known for a long time so that you could have taken appropriate countermeasures in due time (e.g. using a different method instead of a survey). We are looking forward to receiving your master thesis on Thursday. Best regards, The Dean ---- Dear Prof, I would like to ask for a deadline extension for my master thesis. I really got into trouble because of the poor response rate for the survey that I did in the beginning of my thesis. It meant that I had to do the survey again and I spent a lot of time convincing people to participate. All this was not planned... Would be great to hear from you, best regards, Greg'), np.float32(0.31700632))\n",
            "(Document(id='13', metadata={'source': 'student666@MBFH.ch', 'notice': 0.0, 'year': 2018}, page_content='Re:late submission? Dear Ms Orange, you will be granted an extension of two weeks. We understand the critically of the point in time when your sickness occurred. Please submit your thesis on August 4th, midnight. Regards, The Dean ---- Hi Dean, as you can see from the attached medical certificate, I was sick for more than two weeks and would like to ask for a deadline extension for my master thesis. The sickness started three weeks ago when I was starting to write up my results. There were also two (out of 5) evaluation interviews that I had to cancel because of the sickness. This means that I was unable to finish the thesis. Hoping for your understanding, best regards, Olivia'), np.float32(0.3171185))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8BTPPEbcNiA"
      },
      "source": [
        "Here, we connect to an LLM at Groq. To make it work, please get yourself an API key for GROQ and store it as a key on the left side of this notebook...!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "def llm(groq_client, prompt):\n",
        "  chat_completion = groq_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "  )\n",
        "\n",
        "  return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "qZLFmUZ_YPPU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_client = Groq(\n",
        "    api_key=userdata.get('GROQ_API_KEY')\n",
        ")"
      ],
      "metadata": {
        "id": "x9JPwWjLYuJ6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you instruct the Large Language Model what to do:\n",
        "\n",
        "* In the \"system\" part of the prompt, you explain the general task, including the\n",
        "context (i.e. the retrieved information) that the system should rely on. You can pass the content of the retrieved emails by putting \"{context}\" into this part of the prompt\n",
        "* In the \"query\" part of the prompt, you give instruction to make a decision about the new case (as introduced already above, before the retrieval)"
      ],
      "metadata": {
        "id": "Mk4C7brMbDzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = '\\n\\n'.join(list(map(lambda c: c[0].page_content, semantic_results)))\n",
        "prompt = f\"\"\"You are an assistant that helps a study dean to decide about students' request for extending the deadline of their master theses.\n",
        "        The current case is described as follows: {query}.\n",
        "        To decide about the current case, the following historical emails seem to be relevant: {context}. Please make a suggestion whether or not\n",
        "        to grant the deadline extension, including a justification that is based on the given context! If possible, please include quotes from the historical emails\"\"\"\n",
        "print(llm(groq_client, prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVV2T_ufZiNs",
        "outputId": "fc6cb482-3cc1-4667-991d-87e34fe6f656"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the historical emails, I suggest granting the deadline extension for the current student. The justification is as follows:\n",
            "\n",
            "The current student received very late feedback regarding their MRTP and wants to react to it. This situation is unique and unforeseen, similar to Ms. Orange's case, where her sickness occurred at a critical point in time. As the Dean stated in the email to Ms. Orange, \"We understand the critically of the point in time when your sickness occurred.\" This implies that the timing of the event is a crucial factor in the decision-making process.\n",
            "\n",
            "In the current case, the late feedback on the MRTP is a critical event that affects the student's ability to complete the thesis on time. As seen in the previous emails, the Dean has considered the timing and impact of events on the student's thesis progress. For example, in Ms. Smith's case, the Dean stated, \"Since your sickness occurred during a non-critical period of your thesis work and was comparatively short, there was enough time to resolve issues resulting from it.\" This suggests that the Dean takes into account the specific circumstances and their impact on the student's work.\n",
            "\n",
            "In contrast to Mr. Green's case, where the problem was known for a long time and the student could have taken countermeasures, the current student's situation is unexpected and requires a response to the late feedback. As the Dean stated in the email to Mr. Green, \"your case is not one where we can grant an extension - it comes very late and the problem was known for a long time so that you could have taken appropriate countermeasures in due time.\" This is not applicable to the current case, as the student is reacting to unexpected feedback.\n",
            "\n",
            "Therefore, based on the historical context and the unique circumstances of the current case, I suggest granting the deadline extension. The Dean's previous decisions and justifications support the idea that unexpected events, especially those occurring at critical points in time, can be considered valid reasons for a deadline extension. As the Dean stated in the email to Ms. Orange, \"Please submit your thesis on August 4th, midnight,\" indicating a willingness to accommodate unforeseen circumstances.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TWFT5Ew7ZlN0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
